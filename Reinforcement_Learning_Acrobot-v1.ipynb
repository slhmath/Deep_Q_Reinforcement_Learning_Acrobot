{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952182b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee35f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 'Acrobot-v1' environment\n",
    "environ = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de48a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99811082  0.06143937  0.99999579  0.00290001 -0.06177637 -0.06262504]\n"
     ]
    }
   ],
   "source": [
    "environ.seed(1)\n",
    "observe = environ.reset()\n",
    "print(observe) # Represents the 2-d positions of joint-nodes and angular velocities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358162e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.action_space # 0,1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f417dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 0 # any isolated action from environ.reset() has a reward of -1\n",
    "observe, reward, done, info = environ.step(action)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c02113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 20:22:38.996430: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# A neural net that will take as input a state, and output an approximate Q-value for each possible action\n",
    "# It is a guess that the number of hidden layers should match the number of possible actions.\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "in_shape = [environ.observation_space.shape[0]]\n",
    "n_out = environ.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"relu\", input_shape=in_shape),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(n_out, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d9d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes the best way to behave is with a little randomness. Here's our way of introducing that policy.\n",
    "def epsilon_greedy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_out)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2233cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a linked list, or a deque, which will store the experience of our agent. We will perform gradient\n",
    "# descent on a sample of a fixed size from this deque of experiences. Geron calls this a 'replay buffer'\n",
    "from collections import deque\n",
    "rep_buff = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6cebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that samples from the deque of experiences. The sample size is 'batch_size'\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(rep_buff), size=batch_size)\n",
    "    batch = [rep_buff[k] for k in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[j] for experience in batch])\n",
    "        for j in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0558541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that allows the agent to peform one step, using our epsilon_greedy function from states to actions.\n",
    "def one_step(environ, state, epsilon):\n",
    "    action = epsilon_greedy(state, epsilon)\n",
    "    next_state, reward, done, info = environ.step(action)\n",
    "    rep_buff.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0243cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the sample size of experiences at 32. I chose a discount_rate of .99 (the future is important for the acrobot!)\n",
    "# Following Geron's notebook, I chose a learning_rate of 1/100 for the Adam optimizer, which is \n",
    "# a kind of stochastic gradient descent. The training_step function is also borrowed from Geron's notebook.\n",
    "batch_size = 32\n",
    "discount_rate = 0.99\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    max_next_Q_values = np.max(model.predict(next_states), axis=1)\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_out)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df73d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we have a handle on the psuedo-randomness\n",
    "environ.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410be447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 20:22:39.665 Python[10033:2045704] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to (null)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2, Steps: 300, epsilon: 0.993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 20:22:57.769274: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 85, Steps: 249, epsilon: 0.717 It won!\n",
      "Episode: 86, Steps: 216, epsilon: 0.713 It won!\n",
      "Episode: 88, Steps: 199, epsilon: 0.707 It won!\n",
      "Episode: 89, Steps: 272, epsilon: 0.703 It won!\n",
      "Episode: 90, Steps: 212, epsilon: 0.700 It won!\n",
      "Episode: 91, Steps: 175, epsilon: 0.697 It won!\n",
      "Episode: 92, Steps: 228, epsilon: 0.693 It won!\n",
      "Episode: 117, Steps: 191, epsilon: 0.610 It won!\n",
      "Episode: 499, Steps: 300, epsilon: 0.010"
     ]
    }
   ],
   "source": [
    "# Training phase. epsilon will start near 1 then about half-way through the episodes be around .01.\n",
    "# I wanted each episode to have enough steps (N_steps) such that the agent had sufficient time to 'learn'.\n",
    "N_steps = 300 \n",
    "n_steps = N_steps # initialize n_steps\n",
    "for episode in range(500):\n",
    "    observe = environ.reset()\n",
    "    for step in range(N_steps):\n",
    "        epsilon = max(1 - episode /300, 0.01)\n",
    "        observe, reward, done, info = one_step(environ, observe, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "        environ.render()\n",
    "    print(\"\\rEpisode: {}, Steps: {}, epsilon: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    if step < n_steps: \n",
    "        best_weights = model.get_weights() \n",
    "        n_steps = step\n",
    "        best_epsilon = epsilon\n",
    "    if done: \n",
    "        print(\" It won!\")\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a981360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observe = environ.reset()\n",
    "environ.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f3c16b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "# Let's test the agent. Take 1. See 'Test_Run_1.gif' file in repository or READ_ME\n",
    "for step in range(400):\n",
    "    observe, reward, done, info = one_step(environ, observe, 0)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb1c4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "#Take 2. See 2nd .gif file in repository\n",
    "observe = environ.reset()\n",
    "for step in range(400):\n",
    "    observe, reward, done, info = one_step(environ, observe, 0)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "339440a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "#Take 3\n",
    "observe = environ.reset()\n",
    "for step in range(400):\n",
    "    observe, reward, done, info = one_step(environ, observe, 0)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb305422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "#Take 4\n",
    "observe = environ.reset()\n",
    "for step in range(400):\n",
    "    observe, reward, done, info = one_step(environ, observe, 0)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec7ddb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "#Take 5\n",
    "observe = environ.reset()\n",
    "for step in range(400):\n",
    "    observe, reward, done, info = one_step(environ, observe, 0)\n",
    "    if done:\n",
    "        break\n",
    "    environ.render()\n",
    "print(step) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
